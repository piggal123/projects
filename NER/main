import spacy
import pandas as pd
import methods as method
import time
import os


# gets the folder path that the user chose then
# iterates through the files, checking if the file
# is txt. then extracting the text and calling
# the proccessor method. convert the final result from
# dataframe to excel worksheet    
def textExtractor(dirLoc):


    firstTime = True

    counter = 1
    files=os.listdir(dirLoc)
    
    # iterating through the files
    for f in files:
        
        is_file_to_read = False
        path=dirLoc+"\\"+f
       
        file_name=f
        text = " "
       
        # checking the file is a text file
        if(f.endswith("txt")):
 

            # checking if the folder needs to be created
            if(firstTime):
                firstTime = False
                method.create_folder(dirLoc+"\\results\\")
            is_file_to_read=True    
            with open(path,encoding='utf-8',mode="r+") as file:
                text= file.read()
        # checking if there was a txt file
        if(is_file_to_read):
            global firstRun
            

            # checking if it's the first run. if so, loading the nlp
            if(firstRun):
                global nlp
                nlp = spacy.load("he_ner_news_trf")
                firstRun = False
            
  
            proccessor(text,file_name) 
            counter+=1
            print("file " , file_name , " is done.", len(files)-counter , " files left")
            
    global resultDf
    try:
        writer = pd.ExcelWriter(dirLoc+"\\results\\תוצאות.xlsx", engine='xlsxwriter')
        # Convert the dataframe to an XlsxWriter Excel object.
        resultDf.to_excel(writer, sheet_name="תוצאות",index=False)

        # Get the xlsxwriter workbook and worksheet objects.
        worksheet = writer.sheets["תוצאות"]

        # Change the direction for the worksheet.
        worksheet.right_to_left()
        

        # Close the Pandas Excel writer and output the Excel file.
        writer.save()
        
    
        # resting the result dataframe so data won't carry on
        # to the next run
        gloDfs()
        print("סיים")    
    except:
        print("סגור את האקסל והתחל שוב")
            
def proccessor(text,fileName):   
    global resultDf
    textDic={}
    doc = nlp(text)
    # iterating through the entites 
    for entity in doc.ents:

        enText=entity.text
        enLabel=entity.label_
        enCon=str(round(entity._.confidence_score,4))
        entit=enText[1:]
        enti=enText[2:]

        try:

            docO=nlp(entit)
            # iterating through the entity without the first letter
            for ent in docO.ents:
                
                secondEnText=ent.text
                secondEnLabel=ent.label_
                secondEnCon = str(round(ent._.confidence_score,4))
                # checking if the entity label is the same  without the first letter
                if(enLabel==secondEnLabel and secondEnCon >0.7):   
                        enText=secondEnText
                        
        except:
            continue
        try:

            docT=nlp(enti)
             # iterating through the entity without the first two letters
            for en in docT.ents:
                lastEnText=en.text
                lastEnLabel=en.label_
                lastEnCon=str(round(en._.confidence_score,4))
                # checking if the entity label is the same without the first two letters
                if(enLabel==lastEnLabel and lastEnCon > 0.7):
                    enText=lastEnText
                    
        except:
            continue
        try:
            # checking if the word already exists in the dictionary
            if enText in textDic[enLabel]:
              
                continue
            # the word wasn't found in the dictionary
            else:    
                # adding the word to the dictionary
                textDic[enLabel]=textDic[enLabel]+";"+enText
        # it's the first time this label was used, creating the slot in the dictionary        
        except:
            textDic[enLabel]=enText
    persText= " "
    # the tool finds two types of person, checking what he found and combinning it
    if "PERS" in textDic and "PER" in textDic:
        persText=textDic["PERS"]+";"+textDic["PER"]
    elif "PER" in textDic:
        persText = textDic["PER"]
    elif "PERS" in textDic:  
        persText = textDic["PERS"]
    
    # setting up the words for the data frame
    try:
        location=textDic["LOC"]
    except:
        location = " "
        
    try:
        date=textDic["DATE"]
    except:
        date= " "
    try:
        org=textDic["ORG"]
    except:    
        org=" "

    tempDf=pd.DataFrame({"שם הקובץ":[fileName],"entity person":[persText],"entity location":[location],"entity org":[org],"entity date":[date]})
    resultDf=pd.concat([resultDf, tempDf], ignore_index = True, axis = 0)

# setting the dataframes that are going to hold
# the information
def gloDfs():
    global resultDf
    global detailesDf    
    resultDf=pd.DataFrame()
    detailesDf= pd.DataFrame()
    resultDf["שם הקובץ"]= " "
    resultDf["entity person"] = " "
    resultDf["entity location"]= " "
    resultDf["entity org"]= " "
    resultDf["entity date"]= " "

    


def varInit():
    global firstRun
    firstRun = True    
    global nlp
    nlp = " "
    global filesCounter
    filesCounter = 0



def main():

    varInit()
    gloDfs()

    textExtractor(r"C:\Projects\NLP\texts\test")
    
        
if __name__ == "__main__":
    main()      
